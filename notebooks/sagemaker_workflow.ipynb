{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1454f94",
   "metadata": {},
   "source": [
    "# Cancer Prediction with AWS SageMaker\n",
    "\n",
    "This notebook demonstrates a complete, self-contained workflow for cancer prediction using AWS SageMaker:\n",
    "1. Data preparation and upload to S3\n",
    "2. Creating a training script\n",
    "3. Training on SageMaker using scikit-learn\n",
    "4. Deployment to SageMaker endpoint for Lambda and API Gateway integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad71d14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import io\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10513f70-5017-4e91-814a-b2a9cc78bda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up SageMaker environment...\n",
      "Using region: eu-north-1\n",
      "SageMaker Role ARN: arn:aws:iam::876820568174:role/service-role/AmazonSageMaker-ExecutionRole-20250903T123022\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Setting up SageMaker environment...\")\n",
    "\n",
    "# Initialize AWS clients with the correct region\n",
    "region = 'eu-north-1'  # Set to your region\n",
    "sagemaker_session = sagemaker.Session(boto3.Session(region_name=region))\n",
    "role = get_execution_role()\n",
    "\n",
    "print(f\"Using region: {region}\")\n",
    "print(f\"SageMaker Role ARN: {role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd562939",
   "metadata": {},
   "source": [
    "## 1. Prepare and Upload Data\n",
    "\n",
    "First, we'll create a dataset From CSV file which is in S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e27bcf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (455, 30)\n",
      "Test data shape: (114, 30)\n",
      "Feature names: 30 features\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "# Load CSV from S3\n",
    "s3 = boto3.client(\"s3\")\n",
    "bucket_name = \"cancer-prediction-data-arash\"\n",
    "s3_key = \"data/breast_cancer.csv\"  # object path inside the bucket\n",
    "\n",
    "resp = s3.get_object(Bucket=bucket_name, Key=s3_key)\n",
    "df = pd.read_csv(io.BytesIO(resp[\"Body\"].read()))\n",
    "\n",
    "# Determine features and target (expects a 'target' column, otherwise last column is target)\n",
    "if \"target\" in df.columns:\n",
    "    X = df.drop(\"target\", axis=1)\n",
    "    y = df[\"target\"]\n",
    "else:\n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training data shape: {X_train_scaled.shape}\")\n",
    "print(f\"Test data shape: {X_test_scaled.shape}\")\n",
    "print(f\"Feature names: {len(X.columns)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8a6e94",
   "metadata": {},
   "source": [
    "## 3. Train on SageMaker\n",
    "\n",
    "Now we'll train our model Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ded05095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Random Forest model locally...\n",
      "Model Performance:\n",
      "  Accuracy:  0.9649\n",
      "  Precision: 0.9589\n",
      "  Recall:    0.9859\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTraining Random Forest model locally...\")\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Model Performance:\")\n",
    "print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae799785",
   "metadata": {},
   "source": [
    "### Model Deployment\n",
    "Now let's save and deploy our trained model to a SageMaker endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7f04ca-96a6-4e07-915b-e014929d5f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved local model: model.joblib\n",
      "Packaged model into: model.tar.gz\n",
      "Uploaded model artifact to: s3://cancer-prediction-data-arash/cancer-prediction/models/model_1756893471.tar.gz\n",
      "Wrote inference script to: inference.py\n",
      "Deploying endpoint: cancer-prediction-skl-1756893471\n"
     ]
    }
   ],
   "source": [
    "# install/upgrade if you see the pandas/numexpr warning (optional)\n",
    "# !pip install -U numexpr\n",
    "\n",
    "import os, json, time, tarfile\n",
    "import joblib\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "\n",
    "# CONFIG - adjust if needed\n",
    "bucket = 'cancer-prediction-data-arash'\n",
    "prefix = 'cancer-prediction'\n",
    "timestamp = int(time.time())\n",
    "s3 = boto3.client('s3')\n",
    "sm = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()    # already used earlier in notebook\n",
    "endpoint_name = f\"cancer-prediction-skl-{timestamp}\"\n",
    "\n",
    "# 1) Save trained model to joblib (you already trained `model`)\n",
    "local_model_file = \"model.joblib\"\n",
    "joblib.dump(model, local_model_file)\n",
    "print(\"Saved local model:\", local_model_file)\n",
    "\n",
    "# 2) Package model into model.tar.gz (required by SageMaker SKLearnModel)\n",
    "tar_path = \"model.tar.gz\"\n",
    "with tarfile.open(tar_path, \"w:gz\") as tar:\n",
    "    tar.add(local_model_file, arcname=\"model.joblib\")\n",
    "print(\"Packaged model into:\", tar_path)\n",
    "\n",
    "# 3) Upload model.tar.gz to S3\n",
    "s3_key = f\"{prefix}/models/model_{timestamp}.tar.gz\"\n",
    "s3.upload_file(tar_path, bucket, s3_key)\n",
    "model_s3_uri = f\"s3://{bucket}/{s3_key}\"\n",
    "print(\"Uploaded model artifact to:\", model_s3_uri)\n",
    "\n",
    "# 4) Create a minimal inference script (written locally by the notebook)\n",
    "inference_script = \"\"\"\n",
    "import os, joblib, json, numpy as np\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    path = os.path.join(model_dir, \"model.joblib\")\n",
    "    return joblib.load(path)\n",
    "\n",
    "def input_fn(request_body, content_type):\n",
    "    if content_type == \"application/json\":\n",
    "        payload = json.loads(request_body)\n",
    "        # accept {\"instances\": [[...], ...]} or {\"features\": [...]} or raw list\n",
    "        instances = payload.get(\"instances\") or payload.get(\"features\") or payload\n",
    "        arr = np.array(instances)\n",
    "        if arr.ndim == 1:\n",
    "            arr = arr.reshape(1, -1)\n",
    "        return arr\n",
    "    raise ValueError(\"Unsupported content type: \" + content_type)\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    # return probability for positive class and predicted class\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        probs = model.predict_proba(input_data)[:,1].tolist()\n",
    "    else:\n",
    "        probs = model.predict(input_data).tolist()\n",
    "    preds = model.predict(input_data).tolist()\n",
    "    return {\"probabilities\": probs, \"predictions\": preds}\n",
    "\n",
    "def output_fn(prediction, content_type):\n",
    "    if content_type == \"application/json\":\n",
    "        return json.dumps(prediction)\n",
    "    raise ValueError(\"Unsupported content type: \" + content_type)\n",
    "\"\"\"\n",
    "\n",
    "inference_path = \"inference.py\"\n",
    "with open(inference_path, \"w\") as f:\n",
    "    f.write(inference_script)\n",
    "print(\"Wrote inference script to:\", inference_path)\n",
    "\n",
    "# 5) Create SKLearnModel pointing to the S3 model artifact and local inference script\n",
    "sk_model = SKLearnModel(\n",
    "    model_data=model_s3_uri,\n",
    "    role=role,\n",
    "    entry_point=inference_path,\n",
    "    framework_version=\"0.23-1\",   # or another supported sklearn image version\n",
    "    sagemaker_session=sm\n",
    ")\n",
    "\n",
    "# 6) Deploy the model to a SageMaker endpoint\n",
    "print(\"Deploying endpoint:\", endpoint_name)\n",
    "predictor = sk_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    endpoint_name=endpoint_name\n",
    ")\n",
    "print(\"Deployed endpoint:\", endpoint_name)\n",
    "\n",
    "# 7) Quick test invoke using boto3 runtime\n",
    "runtime = boto3.client(\"sagemaker-runtime\")\n",
    "sample = X_test_scaled[0].tolist()   # or X_train_scaled[0]\n",
    "payload = {\"instances\": [sample]}\n",
    "resp = runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(payload)\n",
    ")\n",
    "body = resp[\"Body\"].read().decode()\n",
    "print(\"Raw response:\", body)\n",
    "print(\"Parsed:\", json.loads(body))\n",
    "\n",
    "# Optional: cleanup local temporary files\n",
    "# os.remove(local_model_file); os.remove(tar_path); os.remove(inference_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
